{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7.1: Hacker stats and Darwin's finches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "import scipy.stats as st\n",
    "\n",
    "import numba\n",
    "\n",
    "import bebi103\n",
    "\n",
    "import altair as alt\n",
    "import altair_catplot as altcat\n",
    "\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/git/BE-Bi-103/Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** We start with a little tidying of the data. Think about how you will deal with duplicate measurements of the same bird and make a decision on how those data are to be treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start tidying up the data, we load in the data set and look at its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('../data/finch_beaks.csv', comment='#')\n",
    "\n",
    "# Take a look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for duplicates based on band number and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(g for _, g in df.groupby(['band', 'year']) if len(g) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for ID 19028, the species was identified as fortis and then as scandens, but the beak measurements are the same. For IDs 818, 944, and 945, the rows are exact duplicates. For band 316, the beak depth measurement is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first drop complete duplicates and check that these rows were deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deduped = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_deduped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three rows were indeed deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(g for _, g in df_deduped.groupby(['band', 'year']) if len(g) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['band'] == 19028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['band'] == 316]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes most sense to delete the row for ID 19028 since we have no idea what species the bird is. There are no other measurements in different years for this bird for us to check. For band 316, we could either take the mean of the beak depths or delete one of these rows or delete both rows as well.\n",
    "\n",
    "Let's delete these rows for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with duplicates\n",
    "df_deduped = df_deduped.drop([2057,2178,102,103])\n",
    "\n",
    "# Reset dataframe index\n",
    "df_deduped = df_deduped.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_deduped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check for recordings of the same bird in different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(g for _, g in df_deduped.groupby(['band', 'species']) if len(g) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some birds, they measured the same bird over the different years. For example, taking bird with band 364, both measurements record the same beak length and beak depth. If this is the case for all of this kind of duplicate measurements, this could mean that the bird hasn't grown and that the inital measurement was accurate. If the values did change, maybe the bird grew from a juvenile to an adult. For now, we won't do anything with this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Plot ECDFs of the beak depths of Geospiza scandens in 1975 and in 2012. Then, estimate the mean beak depth in for each of these years with confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's slice out the beak depth values in numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_1975 = ((df_deduped['species'] == 'scandens') &\n",
    "        (df_deduped['year'] == 1975))\n",
    "    \n",
    "inds_2012 = ((df_deduped['species'] == 'scandens') &\n",
    "        (df_deduped['year'] == 2012))\n",
    "\n",
    "depths_1975 = df_deduped.loc[inds_1975, 'beak depth (mm)'].values\n",
    "depths_2012 = df_deduped.loc[inds_2012, 'beak depth (mm)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot ECDFs of the beak depths of Geospiza scandens in 1975 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bebi103.viz.ecdf(depths_1975,\n",
    "                     x_axis_label='Scandens beak depth (mm)',\n",
    "                     color='#4e79a7',\n",
    "                     legend='1975')\n",
    "\n",
    "p = bebi103.viz.ecdf(depths_2012,\n",
    "                     x_axis_label='beak depth (mm)',\n",
    "                     color='#f28e2b',\n",
    "                     legend='2012',\n",
    "                     p=p)\n",
    "\n",
    "p.legend.location = 'bottom_right'\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ECDF, we can see that the mean beak depth in 2012 has increased from what it was in 1975. Now let's estimate the mean beak depth in each of these years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1975 = np.mean(depths_1975)\n",
    "mean_2012 = np.mean(depths_2012)\n",
    "\n",
    "print(\"\"\"\n",
    "Mean beak depth in 1975 (mm): {0:.2f}\n",
    "Mean beak depth in 2012 (mm): {1:.2f}\n",
    "\"\"\".format(mean_1975, mean_2012))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean beak depth in 2012 is greater than the mean beak depth in 1975.\n",
    "\n",
    "We will also estimate confidence intervals. First we have some functions from Justin's tutorial, and then we draw bootstrap samples and calculate the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_bs_sample(data):\n",
    "    \"\"\"\n",
    "    Draw a bootstrap sample from a 1D data set.\n",
    "    \"\"\"\n",
    "    return np.random.choice(data, size=len(data))\n",
    "\n",
    "\n",
    "def draw_bs_reps(data, stat_fun, size=1):\n",
    "    \"\"\"\n",
    "    Draw boostrap replicates computed with stat_fun from 1D data set.\n",
    "    \"\"\"\n",
    "    return np.array([stat_fun(draw_bs_sample(data)) for _ in range(size)])\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def draw_bs_reps_mean(data, size=1):\n",
    "    \"\"\"\n",
    "    Draw boostrap replicates of the mean from 1D data set.\n",
    "    \"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        out[i] = np.mean(draw_bs_sample(data))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bootstrap samples\n",
    "bs_reps_mean_1975 = draw_bs_reps_mean(depths_1975, size=10000)\n",
    "bs_reps_mean_2012 = draw_bs_reps_mean(depths_2012, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% confidence intervals\n",
    "mean_1975_conf_int = np.percentile(bs_reps_mean_1975, [2.5, 97.5])\n",
    "mean_2012_conf_int = np.percentile(bs_reps_mean_2012, [2.5, 97.5])\n",
    "\n",
    "print(\"\"\"\n",
    "Mean beak depth 95% conf int in 1975 (mm): [{0:.2f}, {1:.2f}]\n",
    "Mean beak depth 95% conf int in 2012 (mm): [{2:.2f}, {3:.2f}]\n",
    "\"\"\".format(*(tuple(mean_1975_conf_int) + tuple(mean_2012_conf_int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is slight overlap between these two confidence intervals. \n",
    "\n",
    "We can use the bootstrap replicates to plot the probability distribution of mean beak depths in 1975 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bebi103.viz.ecdf(bs_reps_mean_1975,\n",
    "                     x_axis_label='beak depth (mm)',\n",
    "                     color='#4e79a7',\n",
    "                     legend='1975')\n",
    "p = bebi103.viz.ecdf(bs_reps_mean_2012, color='#f28e2b', legend='2012', p=p)\n",
    "\n",
    "p.legend.location = 'bottom_right'\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some sort of comment for this plot if we include it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Perform a hypothesis test comparing the G. scandens beak depths in 1975 and 2012. Carefully state your null hypothesis, your test statistic, and your definition of what it means to be at least as extreme as the observed test statistic. Comment on the results. It might be interesting to know that a severe drought in 1976 and 1977 resulted in the death of the plants that produce small seeds on the island."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to start with a premutation test, where our null hypothesis is that the two distributions of G. scandens beak depths are exactly the same. As shown in the tutorial, we concentrate the two data sets into one, randomly scramble the order of the combined data set and designate the first $n$ entries in the scrambled array to be one of the test set and the rest to be the other. We use the function in Justin's notes to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_perm_sample(x, y):\n",
    "    \"\"\"Generate a permutation sample.\"\"\"\n",
    "    concat_data = np.concatenate((x, y))\n",
    "    np.random.shuffle(concat_data)\n",
    "    return concat_data[:len(x)], concat_data[len(x):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we choose the difference of means as our test statisitic, where we basically take the means of two premutation data sets and calculate the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_perm_reps_diff_mean(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Generate array of permuation replicates.\n",
    "    \"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        x_perm, y_perm = draw_perm_sample(x, y)\n",
    "        out[i] = np.mean(x_perm) - np.mean(y_perm)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extreme case of this test would be that the absolute value of the difference is larger than the absolute value of  difference of the means of two orginial data sets. Thus, to conduct this test, we draw many replicates and then calculate how many of them have more extreme differences of means than then original sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test statistic for original data set\n",
    "diff_mean = np.abs(np.mean(depths_2012) - np.mean(depths_1975))\n",
    "\n",
    "# Draw replicates\n",
    "perm_reps = np.abs(draw_perm_reps_diff_mean(depths_2012, depths_1975, size=100000))\n",
    "\n",
    "# Compute p-value\n",
    "p_val = np.sum(perm_reps >= diff_mean) / len(perm_reps)\n",
    "\n",
    "print('p-value =', p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good, but we still try to use the difference of the median as our test statistic. The way we do it is quite similar to what we do for the difference of the means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_perm_reps_diff_median(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Generate array of permuation replicates.\n",
    "    \"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        x_perm, y_perm = draw_perm_sample(x, y)\n",
    "        out[i] = np.median(x_perm) - np.median(y_perm)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test statistic for original data set\n",
    "diff_median = np.abs(np.median(depths_2012) - np.median(depths_1975))\n",
    "\n",
    "# Draw replicates\n",
    "perm_reps = np.abs(draw_perm_reps_diff_median(depths_2012, depths_1975, size=100000))\n",
    "\n",
    "# Compute p-value\n",
    "p_val = np.sum(perm_reps >= diff_median) / len(perm_reps)\n",
    "\n",
    "print('p-value =', p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a little worse, and to make sure they are not just two distributions with the same median but different other properties, we use the bootstrap hypothesis to test. The null hypothesis becomes they have exactly the same median, and the test statisic is the difference of the medians. We modify the codes in the notes to shift two data sets so that they have the same median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift data sets\n",
    "total_median = np.median(np.concatenate((depths_2012, depths_1975)))\n",
    "depths_2012_shift = depths_2012 - np.median(depths_2012) + total_median\n",
    "depths_1975_shift = depths_1975 - np.median(depths_1975) + total_median\n",
    "\n",
    "#Plot the ECDFs\n",
    "p = bebi103.viz.ecdf(depths_1975_shift,\n",
    "                     x_axis_label='beak depth (mm)',\n",
    "                     color='#4e79a7',\n",
    "                     legend='1975')\n",
    "p = bebi103.viz.ecdf(depths_2012_shift, color='#f28e2b', legend='2012', p=p)\n",
    "\n",
    "p.legend.location = 'bottom_right'\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do the similar bootstrap test over the data sets with the modified code in the notes, where we can compute the difference of the medians from bootstrap replicates of those two data sets and compare those with the diffrence of the original data sets' medians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_bs_reps_diff_median(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Generate bootstrap replicates with difference of means\n",
    "    as the test statistic.\n",
    "    \"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        out[i] = np.median(draw_bs_sample(x)) - np.median(draw_bs_sample(y))\n",
    "    return out\n",
    "\n",
    "# Generate samples\n",
    "bs_reps = np.abs(draw_bs_reps_diff_median(depths_2012_shift, depths_1975_shift, \n",
    "                                 size=100000))\n",
    "\n",
    "# Compute p-value\n",
    "p_val = np.sum(bs_reps >= diff_median) / len(bs_reps)\n",
    "\n",
    "print('p-value =', p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually pretty high, which indicates that those two species could possibly have similar medians while they have quite different means. Just to make sure, we do the similar test with difference of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift data sets\n",
    "total_mean = np.mean(np.concatenate((depths_2012, depths_1975)))\n",
    "depths_2012_shift = depths_2012 - np.mean(depths_2012) + total_mean\n",
    "depths_1975_shift = depths_1975 - np.mean(depths_1975) + total_mean\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def draw_bs_reps_diff_mean(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Generate bootstrap replicates with difference of means\n",
    "    as the test statistic.\n",
    "    \"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        out[i] = np.mean(draw_bs_sample(x)) - np.mean(draw_bs_sample(y))\n",
    "    return out\n",
    "\n",
    "# Generate samples\n",
    "bs_reps = np.abs(draw_bs_reps_diff_mean(depths_2012_shift, depths_1975_shift, \n",
    "                                 size=100000))\n",
    "\n",
    "# Compute p-value\n",
    "p_val = np.sum(bs_reps >= diff_mean) / len(bs_reps)\n",
    "\n",
    "print('p-value =', p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty good. It is interesting that the mean of G.scandens beak depths changed significantly from 1975 to 2012, which could be because of the death of plants that produce small seeds when they needed to change their beak depth for other food, while the median did not change that much which might illustrate the process of evolution in a way that changes happened first to a certain population that gradually dominated the whole population rather than the whole population had this gradual change through a long time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Devise a measure for the shape of a beak. That is, invent some scalar measure that combines both the length and depth of the beak. Compare this measure between species and through time. (This is very open-ended. It is up to you to define the measure, make relevant plots, compute confidence intervals, and possibly do hypothesis tests to see how shape changes over time and between the two species.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our measure will be the ratio of beak length to beak depth, and we will look at how this measure changes through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of beak length to beak depth\n",
    "df_deduped['beak length to depth ratio'] = df_deduped['beak length (mm)'] / df_deduped['beak depth (mm)']\n",
    "\n",
    "# Take a look\n",
    "df_deduped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We slice out the beak length to depth ratios for each species for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_s = df_deduped.loc[df_deduped['species'] == 'scandens', ['year', 'beak length to depth ratio']]\n",
    "alpha_f = df_deduped.loc[df_deduped['species'] == 'fortis', ['year', 'beak length to depth ratio']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the ECDF of the beak length to depth ratio per species over time. \n",
    "\n",
    "In this plot, we have one thick line representing the ECDF of all the beak length to depth ratios data combined for each species. Then we have thinner lines representing the beak length to depth ratio over time for each species. In this graph, as the lines become darker, the year the data was measured is later in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df_deduped['year'].unique()\n",
    "marker_year = np.linspace(0.2, 0.9, len(years))\n",
    "\n",
    "p = bebi103.viz.ecdf(alpha_f.loc[alpha_f['year'] == years[0], 'beak length to depth ratio'], formal=True, line_width=2, color='#f28e2b', alpha = marker_year[0], legend='fortis')\n",
    "p = bebi103.viz.ecdf(alpha_s.loc[alpha_s['year'] == years[0], 'beak length to depth ratio'], p=p, formal=True, line_width=2, color='#4e79a7', alpha = marker_year[0], legend='scandens')\n",
    "\n",
    "for i in range(len(years) - 1):\n",
    "    p = bebi103.viz.ecdf(alpha_f.loc[alpha_f['year'] == years[i+1], 'beak length to depth ratio'], p=p, formal=True, line_width=2, color='#f28e2b', alpha = marker_year[i+1], legend='fortis')\n",
    "    p = bebi103.viz.ecdf(alpha_s.loc[alpha_s['year'] == years[i+1], 'beak length to depth ratio'], p=p, formal=True, line_width=2, color='#4e79a7', alpha = marker_year[i+1], legend='scandens')\n",
    "\n",
    "p.legend.location = 'bottom_right'\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we have a function to compute the confidence intervals of the mean so that we can compare means of this ratio of different species in different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def mean_conf_int(data, size=1, alpha=5):\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        out[i] = np.mean(draw_bs_sample(data))\n",
    "    return np.percentile(out, [alpha/2, 100-alpha/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate confidence intervals for their mean beak ratio in different years and make them into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the list \n",
    "alpha_s_mean_conf_int = []\n",
    "alpha_f_mean_conf_int = []\n",
    "\n",
    "#Compute the mean conf interval\n",
    "for year in years:\n",
    "    temp = alpha_s.loc[alpha_s['year'] == year, \n",
    "                       'beak length to depth ratio'].values\n",
    "    alpha_s_mean_conf_int.append(np.append(mean_conf_int(temp, size=10000),\n",
    "                                           [np.mean(temp), \n",
    "                                           year]))\n",
    "    temp = alpha_f.loc[alpha_f['year'] == year, \n",
    "                       'beak length to depth ratio'].values\n",
    "    alpha_f_mean_conf_int.append(np.append(mean_conf_int(temp, size=10000),\n",
    "                                           [np.mean(temp), \n",
    "                                           year]))\n",
    "\n",
    "#Construct and combine dataframes \n",
    "df_ratio_mean_conf_int_s = pd.DataFrame(alpha_s_mean_conf_int,columns=['low','high','mean','year'])\n",
    "df_ratio_mean_conf_int_s['species'] = 'scandens'\n",
    "df_ratio_mean_conf_int_f = pd.DataFrame(alpha_f_mean_conf_int,columns=['low','high','mean','year'])\n",
    "df_ratio_mean_conf_int_f['species'] = 'fortis'\n",
    "\n",
    "df_ratio_mean_conf_int = pd.concat([df_ratio_mean_conf_int_s, \n",
    "                                    df_ratio_mean_conf_int_f]).reset_index(drop=True)\n",
    "\n",
    "# Take a look\n",
    "df_ratio_mean_conf_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the mean confidence interval of two species over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_scale = alt.Scale(domain=[1, 1.7])\n",
    "\n",
    "mean = alt.Chart(df_ratio_mean_conf_int,\n",
    "                width=600).mark_line().encode(\n",
    "    x='year:O',\n",
    "    y='mean:Q',\n",
    "    color='species:N'\n",
    ")\n",
    "\n",
    "confidence_interval = alt.Chart(df_ratio_mean_conf_int,\n",
    "                               width=600).mark_area(opacity=0.3).encode(\n",
    "    x='year:O',\n",
    "    y=alt.Y('low:Q', axis=alt.Axis(title='beak length to depth ratio'),scale=axis_scale),\n",
    "    y2='high:Q',\n",
    "    color='species:N'\n",
    ")\n",
    "\n",
    "confidence_interval + mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not hard to tell that there is a difference between to species and may be differences among years. Thus, we conduct hypothesis tests on the difference of means of the ratio in each year and the year after that for each species respectively. The null hypothesis is then that they have the same mean in 1975 and 2012. For convenience, we have a function to do the bootstrap test of the mean differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def test_bs_diff_mean(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Bootstrap test for difference of means\n",
    "    \"\"\"\n",
    "    total_mean = np.mean(np.concatenate((x, y)))\n",
    "    x_shift = x - np.mean(x) + total_mean\n",
    "    y_shift = y - np.mean(y) + total_mean\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        out[i] = np.mean(draw_bs_sample(x_shift)) - np.mean(draw_bs_sample(y_shift))\n",
    "    p_val = np.sum(np.abs(out) >= np.abs(np.mean(x) - np.mean(y))) / size\n",
    "    return p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do the bootstrap test on the ratio in 1975 and in 2012 for G.fortis first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the list \n",
    "alpha_s_p_val = []\n",
    "alpha_f_p_val = []\n",
    "\n",
    "#Compute the p_value\n",
    "for i in range(len(years) - 1):\n",
    "    temp = alpha_s.loc[alpha_s['year'] == years[i], \n",
    "                       'beak length to depth ratio'].values\n",
    "    temp1 = alpha_s.loc[alpha_s['year'] == years[i+1], \n",
    "                       'beak length to depth ratio'].values\n",
    "    alpha_s_p_val.append(np.append(test_bs_diff_mean(temp, temp1, 100000),\n",
    "                                   years[i]))\n",
    "    \n",
    "    temp = alpha_f.loc[alpha_f['year'] == years[i], \n",
    "                       'beak length to depth ratio'].values\n",
    "    temp1 = alpha_f.loc[alpha_f['year'] == years[i+1], \n",
    "                       'beak length to depth ratio'].values\n",
    "    alpha_f_p_val.append(np.append(test_bs_diff_mean(temp, temp1, 100000),\n",
    "                                   years[i]))\n",
    "\n",
    "#Construct and combine dataframes \n",
    "df_ratio_p_val_s = pd.DataFrame(alpha_s_p_val,columns=['p_val','year'])\n",
    "df_ratio_p_val_s['species'] = 'scandens'\n",
    "df_ratio_p_val_f = pd.DataFrame(alpha_f_p_val,columns=['p_val','year'])\n",
    "df_ratio_p_val_f['species'] = 'fortis'\n",
    "\n",
    "df_ratio_p_val = pd.concat([df_ratio_p_val_s, \n",
    "                                    df_ratio_p_val_f]).reset_index(drop=True)\n",
    "\n",
    "# Take a look\n",
    "df_ratio_p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see directly from the data frame that for scandens, the ratio probably didn't change very significantly until 1991, while it might change to some extent in 1975. For fortis, it changed every year except 1987 when it probably didn't change much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Introgressive hybridization occurs when a G. scandens bird mates with a G. fortis bird, and then the offspring mates again with pure G. scandens. This brings traits from G. fortis into the G. scandens genome. As this may be a mode by which beak geometries of G. scandens change over time, it is useful to know how heritable a trait is. Heritability is defined as the ratio of the covariance between parents and offsprings to the variance of the parents alone. To be clear, the heritability is defined as follows.\n",
    "\n",
    "Compute the average value of a trait in a pair of parents.\n",
    "Compute the average value of that trait among the offspring of those parents.\n",
    "Do this for each set of parents/offspring. Using this data set, compute the covariance among all average offspring and the variance among all average parents.\n",
    "This is a more apt definition than, say, the Pearson correlation, because it is a direct comparison between parents and offspring.\n",
    "\n",
    "Heritability data for beak depth for G. fortis and G. scandens can be found here and here, respectively. (Be sure to look at the files before reading them in; they do have different formats.) From these data, compute the heritability of beak depth in the two species, with confidence intervals. How do they differ, and what consequences might this have for introgressive hybridization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs = pd.read_csv('../data/scandens_beak_depth_heredity.csv', comment='#')\n",
    "df_hs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hf = pd.read_csv('../data/fortis_beak_depth_heredity.csv', comment='#')\n",
    "df_hf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the two dataframes have different formats. We need to average the beak length trait from the male and female bird for the fortis dataset (from looking at the header of the scandens dataset, we know this is how they calculate mid_parent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hf['mid_parent'] = (df_hf['Male BD'] + df_hf['Female BD']) / 2\n",
    "df_hf['mid_offspring'] = df_hf['Mid-offspr']\n",
    "df_hf = df_hf.drop(columns=['Male BD', 'Female BD', 'Mid-offspr'])\n",
    "df_hf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's slice out the data as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_par = df_hs['mid_parent'].values\n",
    "hs_offsp = df_hs['mid_offspring'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_par = df_hf['mid_parent'].values\n",
    "hf_offsp = df_hf['mid_offspring'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw bootstrap pairs and define a function to calculate the heritability, which is the ratio of the covariance between parents and offspring to the variance of the parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_bs_pairs(x, y):\n",
    "    \"\"\"\n",
    "    Draw a pairs bootstrap sample.\n",
    "    \"\"\"\n",
    "    inds = np.arange(len(x))\n",
    "    bs_inds = draw_bs_sample(inds)\n",
    "    return x[bs_inds], y[bs_inds]\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def heritability(par, offsp):\n",
    "    '''Heritability is calculated as the ratio of \n",
    "    the covariance between parents and offspring \n",
    "    to the variance of the parents.\n",
    "    r'''\n",
    "    assert len(par) == len(offsp), 'Length of the arrays must be the same.'\n",
    "\n",
    "    return np.sum((par - np.mean(par)) * (offsp - np.mean(offsp))) \\\n",
    "            / np.sum((par - np.mean(par)) * (par - np.mean(par)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def draw_bs_pairs_reps_heritability(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Draw bootstrap pairs replicates.\n",
    "    \"\"\"\n",
    "    out = np.empty(size)\n",
    "    for i in range(size):\n",
    "        out[i] = heritability(*draw_bs_pairs(x, y))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we draw our bootstrap pairs, calculate the heritability, and plot the ECDFs of the beak depth heritability for each bird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_reps = draw_bs_pairs_reps_heritability(hf_par, hf_offsp, size=10000)\n",
    "hs_reps = draw_bs_pairs_reps_heritability(hs_par, hs_offsp, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bebi103.viz.ecdf(hs_reps, formal=True, line_width=2, color='#4e79a7', legend='scandens')\n",
    "p = bebi103.viz.ecdf(hf_reps, p=p, formal=True, line_width=2, color='#f28e2b', legend='fortis')\n",
    "\n",
    "p.legend.location = 'bottom_right'\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the mean and confidence intervals for the heritability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hf = np.mean(hf_reps)\n",
    "mean_hs = np.mean(hs_reps)\n",
    "\n",
    "print(\"\"\"\n",
    "Mean G. fortis heritability for beak depth: {0:.2f}\n",
    "Mean G. scandens heritablity for beak depth: {1:.2f}\n",
    "\"\"\".format(mean_hf, mean_hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_int_hf = np.percentile(hf_reps, [2.5, 97.5])\n",
    "conf_int_hs = np.percentile(hs_reps, [2.5, 97.5])\n",
    "\n",
    "print(\"\"\"\n",
    "G. fortis heritability 95% conf int (mm):   [{0:.2f}, {1:.2f}]\n",
    "G. scandens heritability 95% conf int (mm): [{2:.2f}, {3:.2f}]\n",
    "\"\"\".format(*(tuple(conf_int_hf) + tuple(conf_int_hs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that G. fortis has a higher heritability for beak depth than G. scandens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
