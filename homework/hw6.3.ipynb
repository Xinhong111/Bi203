{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6.3: Analysis of FRAP data (40 pts)Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: Zhiyang and Xinhong did the coding, the whole group discussed together about the models and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.special\n",
    "import scipy.stats as st\n",
    "import statsmodels.tools.numdiff as smnd\n",
    "\n",
    "import bebi103\n",
    "\n",
    "import altair as alt\n",
    "import bokeh.io\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model given by the problem set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "I_\\mathrm{norm}(t) \\equiv I(t)/I_0 &= \n",
    "f_f\\left(1 - f_b\\,\\frac{4 \\mathrm{e}^{-k_\\mathrm{off}t}}{d_x d_y}\\,\\psi_x(t)\\,\\psi_y(t)\\right),\\\\[1mm]\n",
    "\\text{where } \\psi_i(t) &= \\frac{d_i}{2}\\,\\mathrm{erf}\\left(\\frac{d_i}{\\sqrt{4Dt}}\\right)\n",
    "-\\sqrt{\\frac{D t}{\\pi}}\\left(1 - \\mathrm{e}^{-d_i^2/4Dt}\\right),\n",
    "\\end{align}\n",
    "\n",
    "We will first turn the equations of the model into functions, where we want to compute the $I_{norm}(t)$ first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluo_bleach_psi(dx, D, t):\n",
    "    '''Specifies psi in Inorm equation.'''\n",
    "    return dx / 2 * scipy.special.erf(dx / np.sqrt(4 * D * t)) - \\\n",
    "           np.sqrt(D * t / np.pi) * (1 - np.exp(-np.power(dx,2) / (4 * D * t)))\n",
    "\n",
    "def fluo_bleach_cal(t, f_b, f_f, k_off, D, dx=40*0.138, dy=40*0.138):\n",
    "    '''Piecewise equation for Inorm, where before time 0, Inorm=1.'''\n",
    "    # Before photobleaching happens the normalized intensity is 1\n",
    "    if t < 0:\n",
    "        return 1\n",
    "    # When photobleaching happens, take the limit of t approaching 0\n",
    "    elif t == 0:\n",
    "        return f_f * (1 - f_b)\n",
    "    # Other time points can be computed as shown above\n",
    "    else:\n",
    "        psi_x = fluo_bleach_psi(dx, D, t)\n",
    "        psi_y = fluo_bleach_psi(dy, D, t)\n",
    "        return f_f * (1 - f_b * 4 * np.exp(-k_off * t) / (dx * dy) * psi_x * psi_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset\n",
    "df = pd.read_csv('../data/hw_4.1_frap_image_processing_results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's plot the data and get an idea of how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the trials\n",
    "alt.Chart(df\n",
    "    ).mark_line(\n",
    "        strokeJoin='bevel'\n",
    "    ).encode(\n",
    "        x=alt.X('time:Q', title='time (s)'),\n",
    "        y=alt.Y('normalized_intensity:Q', title='normalized intensity'),\n",
    "        color='trial:N',\n",
    "        order='time:Q'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here we use the normalized intensity which is normalized by the mean of the intensity before the photobleaching for each trial respectively. We think it is better to use this to compare among trials.\n",
    "\n",
    "Even though this may cause problems like neglected $I_0$ in the model, we assume the reason to have another parameter $I_0$ is to account for the measurement noise, which could also be done by using a normal distribution with mean of $I_{norm}(t, f_b, f_f, k_{off}, D)$ and standard deviation of some $\\sigma$. This is actually equivalent to having a normal distribution $I_0$ with the mean of mean intensity by time 0 for each trial and some standard deviation. The only problem is that $\\sigma$ could be different among trials, but we think it is appropriate to assume similar measurement noise here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the data and consider the definition of parameters, we hypothesize the distribution of different parameters:\n",
    "\n",
    "We think the data $I(t)$ in general would follow a normal distribution where the mean is $I_{norm}(t, f_b, f_f, k_{off}, D)$ and the standard deviation is $\\sigma$. \n",
    "\n",
    "* $f_b$ is the fraction of bleached fluorophores, it has the bound of 0~1, and it shold be close to 1, so we speculate it would follow a beta distribution.  \n",
    "* $f_f$ is the fraction of total fluorescent species left after photobleaching, it would be similar to fb, should also follow beta distribution.  \n",
    "* $k_{off}$ is chemical rate constant, the situation is similar to D, so it should also lies in a lognormal distribution.  \n",
    "* $D$ is the diffusion coefficent, we assume it could be similar across trials, which indicates  a sharp peak, also it has a lower bound (0), but not an upper bound. So lognormal distribution would best to describe D.  \n",
    "* $\\sigma$ accounts for the measurement noise which usually follows a half normal distribution. \n",
    "\n",
    "\n",
    "For $f_b$ and $f_f$, we basically look at the shape of the beta distribution and get the parameters we think is good to describe the situation here. For $k_{off}$ and $D$, we have no idea about what they should be like, so we choose a heavily tailed log normal distribution with some means we guess from the data. we set the standard deviation to 0.02 since the data is actually pretty clean.  \n",
    "\n",
    "After getting the general idea of distribution of each parameters, let's specify each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the distributions for each parameters in the model\n",
    "def data_prior_pred(t, n_ppc_samples=1):\n",
    "    '''\n",
    "    Samples parameter values according to the prior and generates\n",
    "    data y at the values given in x.\n",
    "    '''\n",
    "    sigma = np.abs(np.random.normal(0, 0.02, size=n_ppc_samples))\n",
    "    k_off = np.random.lognormal(np.log(0.2), 0.5, size=n_ppc_samples)\n",
    "    D = np.random.lognormal(np.log(0.1), 0.75, size=n_ppc_samples)\n",
    "    fb = np.random.beta(6, 3, size=n_ppc_samples)\n",
    "    ff = np.random.beta(15, 2, size=n_ppc_samples)\n",
    "    \n",
    "    data = np.array([np.random.normal(fluo_bleach_cal(t0, fb, ff, k_off, D),\n",
    "                                      sigma, size=1) \n",
    "                        for t0 in t])\n",
    "    return np.concatenate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our prior is:\n",
    "\n",
    "$$f_b\\sim Beta(6,3)$$\n",
    "$$f_f\\sim Beta(15,2)$$\n",
    "$$k_{off}\\sim LogNorm(\\ln(0.2), 0.5)$$\n",
    "$$D\\sim LogNorm(\\ln (0.1), 0.75)$$\n",
    "$$\\sigma \\sim Norm(0, 0.01)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the likelihood is:\n",
    "\n",
    "$$I(t) \\sim Norm(I_{norm}(t, f_b, f_f, k_{off}, D), \\sigma)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Trial0 to do a prior predictive check! First let's slice out the data for trial0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice out the data belongs to trial0\n",
    "t0 = df.loc[df['trial']==0, 'time'].values\n",
    "v0 = df.loc[df['trial']==0, 'normalized_intensity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot several trials of the simulated data using the prior predictive check against the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior check use the data in trial0\n",
    "colors = bokeh.palettes.all_palettes['Category20'][20]\n",
    "\n",
    "p = bokeh.plotting.figure(height=300, width=450,\n",
    "                          x_axis_label='Time(s)',\n",
    "                          y_axis_label='Normalized intensity')\n",
    "\n",
    "# Plot simulated data\n",
    "for i in range(20):\n",
    "    p.line(t0, data_prior_pred(t0), color=colors[i],\n",
    "             alpha=0.8)\n",
    "\n",
    "# Plot original data\n",
    "p.circle(t0, v0, color='black', size=4)\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black dots are the original data we acquired from the dataset, the colored lines are the simulated data generated from our prior. We could see that our prior cover most of the real data, which indicates our prior seems reasonable. However, since we are going to use this same prior for all the trials, it is possible that they didn't fit with the data in other trial very well, so when we do prior check for each trial, we may have to go back to tune the parameters of our model. But let's move on to following analysis of Trial0 with this prior now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify the log-likelihood and log-prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the log_prior\n",
    "def log_prior(params):\n",
    "    \n",
    "    if (params < 0).any():\n",
    "        return -np.inf\n",
    "    \n",
    "    fb, ff, k_off, D, sigma = params\n",
    "\n",
    "    log_prior = st.beta.logpdf(fb, 6, 3)\n",
    "    log_prior += st.beta.logpdf(ff, 15, 1)\n",
    "    log_prior += st.lognorm.logpdf(k_off, 0.5, loc=0, scale=0.2)\n",
    "    log_prior += st.lognorm.logpdf(D, 0.75, loc=0, scale=0.3)\n",
    "    log_prior += st.halfnorm.logpdf(sigma, 0, 0.01)\n",
    "    \n",
    "    return log_prior\n",
    "\n",
    "# Specify the log_likelihood\n",
    "def log_like(params, t, y):\n",
    "    fb, ff, k_off, D, sigma = params\n",
    "    \n",
    "    log_like = np.sum([st.norm.logpdf(y0, fluo_bleach_cal(t0, fb, ff, k_off, D), sigma)\n",
    "                       for y0, t0 in zip(y, t)])\n",
    "    \n",
    "    return log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to calculate the log-posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the posterior using the prior and likelihood\n",
    "def log_post(params, t, y):\n",
    "    return log_prior(params) + log_like(params, t, y)\n",
    "\n",
    "def neg_log_post(params, t, y):\n",
    "    return -log_post(params, t, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 5 parameters in total, it's hard to plot the posterior. So we will directly go ahead and find the MAP values.  \n",
    "\n",
    "After several trials of finding MAP values using different methods, we find that 'SLSQP' works the best for us. We are not quite sure about why this method works but not the others. For instance, when we use 'Powell' method Justin used in the Tutorial, it gives us some weird number larger than 200.\n",
    "\n",
    "Let's compute the MAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify arguments\n",
    "args_0 = (t0, v0)\n",
    "\n",
    "# Choose initial conditiona\n",
    "params_0 = [0.9, 0.9, 0.2, 0.3, 0.01]\n",
    "\n",
    "# Compute the MAP values\n",
    "opt_res_0 = scipy.optimize.minimize(neg_log_post, params_0, args=args_0,\n",
    "                                    method='SLSQP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And take a look at the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the Map values \n",
    "popt_0 = opt_res_0.x\n",
    "print(popt_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter values look reasonable based on the definition of parameters and our intuition for their distribution. So let's plug these paramters into our model and plot the simulated line using the data sample from this generative model. We can also plot the measured data in the same plot to compare the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a posterior check with the real data\n",
    "\n",
    "# Plug in the paramters we get and generate some simulated data\n",
    "def data_prior_pred_0(t, n_ppc_samples=1):\n",
    "    fb, ff, k_off, D, sigma = popt_0\n",
    "    data = np.array([np.random.normal(fluo_bleach_cal(t0, fb, ff, k_off, D),\n",
    "                                      sigma, size=1) \n",
    "                        for t0 in t])\n",
    "    return np.concatenate(data)\n",
    "\n",
    "\n",
    "colors = bokeh.palettes.all_palettes['Category20'][20]\n",
    "\n",
    "p = bokeh.plotting.figure(height=300, width=450,\n",
    "                          x_axis_label='Time(s)',\n",
    "                          y_axis_label='Normalized intensity')\n",
    "\n",
    "# Plot simulated data for 20 'simulated trials'\n",
    "for i in range(20):\n",
    "    p.line(t0, data_prior_pred_0(t0), color=colors[i],\n",
    "             alpha=0.8)\n",
    "\n",
    "# Plot original data\n",
    "p.circle(t0, v0, color='black', size=4)\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black dots are the original data we acquired from the dataset, the colored lines are several trials of simulated data generated from our posterior. We could see they fit with each other quite well, indicating that our estimation of paramters are quite reasonable. So let's move on and calculate the 95% credible region for these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hes = smnd.approx_hess(popt_0, log_post, args=args_0)\n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov = -np.linalg.inv(hes)\n",
    "\n",
    "# Look at it\n",
    "for i in range(5):\n",
    "    print(abs(cov[i][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put these values into a nice form so we can read the credible regions conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience...\n",
    "fb_MAP_0, ff_MAP_0, k_off_MAP_0, D_MAP_0, sigma_MAP_0  = popt_0\n",
    "\n",
    "# Print results\n",
    "print(\"\"\"\n",
    "Most probable parameters for Trial0\n",
    "-----------------------------------\n",
    "fb_MAP_0 = {0:.2f} Â± {1:.2f} \n",
    "ff_MAP_0 = {2:.3f} Â± {3:.3f}\n",
    "k_off_MAP_0 = {4:.3f} Â± {5:.3f}\n",
    "D_MAP_0 = {6:.3f} Â± {7:.3f}\n",
    "sigma_MAP_0 = {8:.3f} Â± {9:.3f}\n",
    "\"\"\".format(fb_MAP_0, 2* np.sqrt(abs(cov[0, 0])), \n",
    "           ff_MAP_0, 2* np.sqrt(abs(cov[1, 1])), \n",
    "           k_off_MAP_0, 2* np.sqrt(abs(cov[2, 2])),\n",
    "           D_MAP_0, 2* np.sqrt(abs(cov[3, 3])),\n",
    "           sigma_MAP_0, 2* np.sqrt(abs(cov[4, 4]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the values of the parameters and their 95% credible region! They looks fairy reasonable. Let's do the simailar analysis on all the other trials.   \n",
    "\n",
    "To perform the similar method on all the trial, we would like to summarize the function into several blocks, and call the block for each trial to perform all the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we write a function to perform and plot the prior predictive check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data_prior_check(i, N_iter=20):\n",
    "    '''Performs 20 simulations from the prior and plots\n",
    "    the simulated data against the actual data for trial i.'''\n",
    "    ti = df.loc[df['trial']==i, 'time'].values\n",
    "    vi = df.loc[df['trial']==i, 'normalized_intensity'].values\n",
    "    \n",
    "    colors = bokeh.palettes.all_palettes['Category20'][20]\n",
    "\n",
    "    p1 = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='time(s)',\n",
    "                              y_axis_label='intensity',\n",
    "                              title='prior')\n",
    "\n",
    "    # Plot simulated data\n",
    "    for i in range(N_iter):\n",
    "        p1.line(ti, data_prior_pred(ti), color=colors[i], alpha=0.8)\n",
    "\n",
    "    # Plot original data\n",
    "    p1.circle(ti, vi, color='black', size=4)\n",
    "      \n",
    "    return ti, vi, p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to compute MAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Map_values(i, ti, vi, params_i):\n",
    "    '''Computes MAP values using SLSQP method.'''\n",
    "    \n",
    "    # Choose initial condition\n",
    "    params = params_i\n",
    "\n",
    "    # Compute the MAP values\n",
    "    opt_res_i = scipy.optimize.minimize(neg_log_post, \n",
    "                                        params, \n",
    "                                        args=(ti, vi),\n",
    "                                        method='SLSQP')\n",
    "    #return Map_values\n",
    "    return opt_res_i.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write functions to plot the simulated line based on the MAP parameters and original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prior_pred_back(ti, popt_i, n_ppc_samples=1):\n",
    "    '''Predictive check using MAP values'''\n",
    "    fb, ff, k_off, D, sigma = popt_i\n",
    "    data = np.array([np.random.normal(fluo_bleach_cal(t, fb, ff, k_off, D),\n",
    "                                      sigma, \n",
    "                                      size=1) \n",
    "                     for t in ti])\n",
    "    \n",
    "    return np.concatenate(data)\n",
    "\n",
    "def plot_best_fit_line(i, ti, vi, popt_i, N_iter=20):\n",
    "    '''Plots simulated data against original data.'''\n",
    "    colors = bokeh.palettes.all_palettes['Category20'][20]\n",
    "\n",
    "    p2 = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='time(s)',\n",
    "                              y_axis_label='intensity',\n",
    "                              title = 'MAP predictive check')\n",
    "\n",
    "    # Plot simulated data\n",
    "    for i in range(N_iter):\n",
    "        p2.line(ti, data_prior_pred_back(ti, popt_i), color=colors[i],\n",
    "               alpha=0.8)\n",
    "\n",
    "    # Plot original data\n",
    "    p2.circle(ti, vi, color='black', size=4)\n",
    "    \n",
    "    return p2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function to compute the 95% credible regions for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_parameter_with_95(i, popt_i, args_i):\n",
    "    '''Calculates the 95% credible regions for the parameters and prints them\n",
    "    in a formatted way.'''\n",
    "    # Using very small step size due to some values close to upper bound.\n",
    "    hes = smnd.approx_hess(popt_i, log_post, args=args_i,  epsilon=1e-8)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    cov = -np.linalg.inv(hes)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Trial\", i)\n",
    "    print(\"\"\"\n",
    "    Most probable parameters \n",
    "    ------------------------\n",
    "    fb_MAP = {0:.2f} Â± {1:.2f} \n",
    "    ff_MAP = {2:.3f} Â± {3:.3f}\n",
    "    k_off_MAP = {4:.3f} Â± {5:.3f}\n",
    "    D_MAP = {6:.3f} Â± {7:.3f}\n",
    "    sigma_MAP = {8:.3f} Â± {9:.3f}\n",
    "    \n",
    "    --------------------------------------------\n",
    "    \"\"\".format(popt_i[0], 2* np.sqrt(abs(cov[0, 0])), \n",
    "               popt_i[1], 2* np.sqrt(abs(cov[1, 1])), \n",
    "               popt_i[2], 2* np.sqrt(abs(cov[2, 2])),\n",
    "               popt_i[3], 2* np.sqrt(abs(cov[3, 3])),\n",
    "               popt_i[4], 2* np.sqrt(abs(cov[4, 4]))))\n",
    "\n",
    "    #return fb, ff, k_off, D, sigma\n",
    "    return [popt_i[0], 2* np.sqrt(abs(cov[0, 0]))], [popt_i[1], 2* np.sqrt(abs(cov[1, 1]))], \\\n",
    "            [popt_i[2], 2* np.sqrt(abs(cov[2, 2]))], [popt_i[3], 2* np.sqrt(abs(cov[3, 3]))], \\\n",
    "            [popt_i[4], 2* np.sqrt(abs(cov[4, 4]))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the function blocks we need for the analysis, since we already performed analysis on Trial0, let's test the function with Trial0 to make sure everything is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function blocks on Trial0\n",
    "\n",
    "# Perform prior predictive check\n",
    "ti, vi, p1 = slice_data_prior_check(i)\n",
    "\n",
    "# Compute MAP values\n",
    "popt_i = compute_Map_values(i, ti, vi, [0.9, 0.9, 0.2, 0.3, 0.01])\n",
    "\n",
    "# Plot MAP predictive check against data\n",
    "args_i = (ti, vi)\n",
    "p2 = plot_best_fit_line(i, ti, vi, popt_i)\n",
    "\n",
    "# Compute 95% credible regions\n",
    "result_i = present_parameter_with_95(i, popt_i, args_i)\n",
    "fb = result_i[0][:]\n",
    "ff = result_i[1][:]\n",
    "k_off = result_i[2][:]\n",
    "D = result_i[3][:]\n",
    "sigma = result_i[4][:]\n",
    "\n",
    "# Display plots\n",
    "bokeh.io.show(bokeh.layouts.gridplot([p1, p2], ncols=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black dots in both graphs are the measured data. Left graphs plot the simulated data from prior, right graphs plot simulated data from posterior. The result is the same as what we get from direct operation on trial0, let's move on and use this on other trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function blocks on Trial0\n",
    "\n",
    "# Perform prior predictive check\n",
    "ti, vi, p1 = slice_data_prior_check(i)\n",
    "\n",
    "# Compute MAP values\n",
    "popt_i = compute_Map_values(i, ti, vi, [0.9, 0.9, 0.2, 0.3, 0.01])\n",
    "\n",
    "# Plot MAP predictive check against data\n",
    "args_i = (ti, vi)\n",
    "p2 = plot_best_fit_line(i, ti, vi, popt_i)\n",
    "\n",
    "# Compute 95% credible regions\n",
    "result_i = present_parameter_with_95(i, popt_i, args_i)\n",
    "fb = result_i[0][:]\n",
    "ff = result_i[1][:]\n",
    "k_off = result_i[2][:]\n",
    "D = result_i[3][:]\n",
    "sigma = result_i[4][:]\n",
    "\n",
    "# Display plots\n",
    "bokeh.io.show(bokeh.layouts.gridplot([p1, p2], ncols=2))\n",
    "\n",
    "# For the rest\n",
    "for i in range(7):\n",
    "    # Perform prior predictive check\n",
    "    ti, vi, p1 = slice_data_prior_check(i+1)\n",
    "    \n",
    "    # Compute MAP values\n",
    "    popt_i = compute_Map_values(i+1, ti, vi, [0.9, 0.9, 0.2, 0.3, 0.01])\n",
    "    \n",
    "    # Plot MAP predictive check against data\n",
    "    args_i = (ti, vi)\n",
    "    p2 = plot_best_fit_line(i+1, ti, vi, popt_i)\n",
    "    \n",
    "    # Compute 95% credible regions\n",
    "    result_i = present_parameter_with_95(i+1, popt_i, args_i)\n",
    "    fb = np.vstack([fb, result_i[0][:]])\n",
    "    ff = np.vstack([ff, result_i[1][:]])\n",
    "    k_off = np.vstack([k_off, result_i[2][:]])\n",
    "    D = np.vstack([D, result_i[3][:]])\n",
    "    sigma = np.vstack([sigma, result_i[4][:]])\n",
    "    \n",
    "    # Display plots\n",
    "    bokeh.io.show(bokeh.layouts.gridplot([p1, p2], ncols=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue in Trial2 where the MAP values for $f_f$ is too close to 1 so that we cannot have an accurate estimate of upper bound of 95% credible region. We change the step of Hes matrix to be very small but there is still something wrong in the result of Trial2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running through each trial, we would like to look at all the parameters and plot the jitter plot for all the parameters with 95% credible regions.\n",
    "\n",
    "Let's start with $f_b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at fb\n",
    "print(\"fb[value, 95% credible region]:\", '\\n', fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to first put the estimate for $f_b$ in every trial into a dataframe for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the estimate for fb in every trial into a dataframe\n",
    "fb_estimate = pd.DataFrame(data={'value': fb[:,0], '95%': fb[:,1]})\n",
    "fb_estimate.insert(0, 'trial', range(0, 0 + len(fb_estimate)))\n",
    "\n",
    "# Take a look\n",
    "fb_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the estimate of fb with 95% credible regions to get a better sense of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice out data for convenience\n",
    "xs = fb_estimate['trial']\n",
    "ys = fb_estimate['value']\n",
    "yerrs = fb_estimate['95%']\n",
    "\n",
    "# Plot the points\n",
    "p_fb = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='trial',\n",
    "                              y_axis_label='value',\n",
    "                              title = 'estimate of fb with 95% credible regions')\n",
    "\n",
    "p_fb.circle(xs, ys, color='green', size=5, line_alpha=0)\n",
    "\n",
    "\n",
    "# create the coordinates for the errorbars\n",
    "err_xs = []\n",
    "err_ys = []\n",
    "\n",
    "for x, y, yerr in zip(xs, ys, yerrs):\n",
    "    err_xs.append((x, x))\n",
    "    err_ys.append((y - yerr, y + yerr))\n",
    "\n",
    "# plot them\n",
    "p_fb.multi_line(err_xs, err_ys, color='green')\n",
    "\n",
    "bokeh.plotting.show(p_fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the similar plot on the other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the estimate of $f_f$ with 95% credible regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the estimate for ff in every trial into a dataframe\n",
    "ff_estimate = pd.DataFrame(data={'value': ff[:,0], '95%': ff[:,1]})\n",
    "ff_estimate.insert(0, 'trial', range(0, 0 + len(ff_estimate)))\n",
    "ff_estimate\n",
    "\n",
    "# for convenience..\n",
    "xs = ff_estimate['trial']\n",
    "ys = ff_estimate['value']\n",
    "yerss = ff_estimate['95%']\n",
    "\n",
    "# plot the points\n",
    "p_ff = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='trial',\n",
    "                              y_axis_label='value',\n",
    "                              title = 'estimate of ff with 95% credible regions')\n",
    "\n",
    "p_ff.circle(xs, ys, color='green', size=5, line_alpha=0)\n",
    "\n",
    "\n",
    "# create the coordinates for the errorbars\n",
    "err_xs = []\n",
    "err_ys = []\n",
    "\n",
    "for x, y, yerr in zip(xs, ys, yerrs):\n",
    "    err_xs.append((x, x))\n",
    "    err_ys.append((y - yerr, y + yerr))\n",
    "\n",
    "# plot them\n",
    "p_ff.multi_line(err_xs, err_ys, color='green')\n",
    "\n",
    "bokeh.plotting.show(p_ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the estimate of $k_{off}$ with 95% credible regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the estimate of k_off with 95% credible region\n",
    "\n",
    "# Put the estimate for k_off in every trial into a dataframe\n",
    "k_off_estimate = pd.DataFrame(data={'value': k_off[:,0], '95%': k_off[:,1]})\n",
    "k_off_estimate.insert(0, 'trial', range(0, 0 + len(k_off_estimate)))\n",
    "k_off_estimate\n",
    "\n",
    "# for convenience..\n",
    "xs = k_off_estimate['trial']\n",
    "ys = k_off_estimate['value']\n",
    "yerss = k_off_estimate['95%']\n",
    "\n",
    "# plot the points\n",
    "p_k_off = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='trial',\n",
    "                              y_axis_label='value',\n",
    "                              title = 'estimate of k_off with 95% credible regions')\n",
    "\n",
    "p_k_off.circle(xs, ys, color='blue', size=5, line_alpha=0)\n",
    "\n",
    "\n",
    "# create the coordinates for the errorbars\n",
    "err_xs = []\n",
    "err_ys = []\n",
    "\n",
    "for x, y, yerr in zip(xs, ys, yerrs):\n",
    "    err_xs.append((x, x))\n",
    "    err_ys.append((y - yerr, y + yerr))\n",
    "\n",
    "# plot them\n",
    "p_k_off.multi_line(err_xs, err_ys, color='blue')\n",
    "\n",
    "bokeh.plotting.show(p_k_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something seems wrong in the Trial2, we proceed and will get back to it. Plot the estimate of D with 95% credible region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the estimate of D with 95% credible region\n",
    "\n",
    "# Put the estimate for D in every trial into a dataframe\n",
    "D_estimate = pd.DataFrame(data={'value': D[:,0], '95%': D[:,1]})\n",
    "D_estimate.insert(0, 'trial', range(0, 0 + len(D_estimate)))\n",
    "D_estimate\n",
    "\n",
    "# for convenience..\n",
    "xs = D_estimate['trial']\n",
    "ys = D_estimate['value']\n",
    "yerss = D_estimate['95%']\n",
    "\n",
    "# plot the points\n",
    "p_D = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='trial',\n",
    "                              y_axis_label='value',\n",
    "                              title = 'estimate of D with 95% credible region')\n",
    "\n",
    "p_D.circle(xs, ys, color='orange', size= 5, line_alpha=0)\n",
    "\n",
    "\n",
    "# create the coordinates for the errorbars\n",
    "err_xs = []\n",
    "err_ys = []\n",
    "\n",
    "for x, y, yerr in zip(xs, ys, yerrs):\n",
    "    err_xs.append((x, x))\n",
    "    err_ys.append((y - yerr, y + yerr))\n",
    "\n",
    "# plot them\n",
    "p_D.multi_line(err_xs, err_ys, color='black')\n",
    "\n",
    "bokeh.plotting.show(p_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error bar in D plot looks very small (the little black lines) because of the scale, but we can still see that the Trial2 is kind of strange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the estimate of sigma with 95% credible region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the estimate of sigma with 95% credible region\n",
    "\n",
    "# Put the estimate for sigma in every trial into a dataframe\n",
    "sigma_estimate = pd.DataFrame(data={'value': sigma[:,0], '95%': sigma[:,1]})\n",
    "sigma_estimate.insert(0, 'trial', range(0, 0 + len(sigma_estimate)))\n",
    "sigma_estimate\n",
    "\n",
    "# for convenience..\n",
    "xs = sigma_estimate['trial']\n",
    "ys = sigma_estimate['value']\n",
    "yerss = sigma_estimate['95%']\n",
    "\n",
    "# plot the points\n",
    "p_sigma = bokeh.plotting.figure(height=300, width=450,\n",
    "                              x_axis_label='trial',\n",
    "                              y_axis_label='value',\n",
    "                              title = 'estimate of sigma with 95% credible region')\n",
    "\n",
    "p_sigma.circle(xs, ys, color='purple', size=5, line_alpha=0)\n",
    "\n",
    "\n",
    "# create the coordinates for the errorbars\n",
    "err_xs = []\n",
    "err_ys = []\n",
    "\n",
    "for x, y, yerr in zip(xs, ys, yerrs):\n",
    "    err_xs.append((x, x))\n",
    "    err_ys.append((y - yerr, y + yerr))\n",
    "\n",
    "# plot them\n",
    "p_sigma.multi_line(err_xs, err_ys, color='purple')\n",
    "\n",
    "bokeh.plotting.show(p_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize all the parameters together by trial with their 95% confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(bokeh.layouts.gridplot([p_fb, p_ff, p_k_off, p_D, p_sigma], ncols=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is something different in Trial2 from others. We are not sure why this is happening, possbily due to the very high estimate of $f_f$ but other parameters are also very different from those in other trials, which should be similar. It might be because of the MAP optimizer or coding of the posterior, but we did not find anything wrong in the function. We would like to just put Trial2 aside if we will further analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -v -p numpy,pandas,statsmodels,altair,scipy,bokeh,jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "**Grade: 36.5/40**\n",
    "\n",
    "Very nice work overall!  Clearly explanations and work flow.\n",
    "\n",
    "Some comments:\n",
    "- Just because you use normalized data doesnât mean that you can ignore $I_0$ in your model.  Your $I_0$ would simply have a change of units such that a reasonable prediction for it would be approximately 1.  You even mention that you should have another parameter $I_0$ to account for measurement noise, but then donât make a prior on it. (-2 pts)\n",
    "- DO NOT PLOT DATA WITH PRIOR PREDICTIVE CHECKS.  Itâs not about whether the data match the prior predictive checks, but rather whether the prior predictive checks on their own give something reasonable that you would expect from your model. (-1 pt)\n",
    "- In your PPC plot, it is unclear what the smoother colored lines are compared to the noisier ones.\n",
    "- 95% confidence intervals are technically 1.96 x sqrt(cov), not 2 times.  Also you SHOULD NOT take absolute value of covariance matrix...  Diagonal values should always be positive! (-0.5 pt)\n",
    "- it is interesting that you saw Trial 2 giving outliers for parameter estimates, where we observed Trial 7 being an outlier.  Could just be due to the priors you set up, or I just didnât catch somethingâ¦\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
