{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.2: Outliers in FRET binding curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: Zhiyang did this problem, the whole group discussed together for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import altair_catplot as altcat\n",
    "\n",
    "import bebi103\n",
    "\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "import bokeh.models\n",
    "import bokeh.layouts\n",
    "bokeh.io.output_notebook()\n",
    "color_palette=['#4e79a7', '#f28e2b', '#e15759', '#76b7b2', '#59a14f', '#edc948', '#b07aa1', '#ff9da7', '#9c755f', '#bab0ac']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the model, we want to have a general idea about what the data look like, so we load the data set into data frame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "df = pd.read_csv('../data/fret_binding_curve.csv', comment='#')\n",
    "\n",
    "# Take a look\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tidy data frame, and we can have an idea about the order of magnitude of the fluorescence measurement is about $10^5$. Looking at the definition given in the problem, the dissociation costant $K_d$ is:\n",
    "\n",
    "\\begin{equation}\\tag{1}\n",
    "K_d = \\frac{c_a c_b}{c_{ab}}\n",
    "\\end{equation}\n",
    "\n",
    ", where $c_i$ is the concentration of species $i$. So basically from this definition, we would know that $K_d$ cannot be negative since there is no negative concentration, and it can range from 0 to infinity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take a look at the way the experiment is done, the data we have is the concentration of $a$ and $b$, along with the fluorescence readings. And we have the equation as following:\n",
    "\n",
    "\\begin{equation} \\tag{2}\n",
    "F = \\hat{f}_0(c_a^0 - c_{ab}) + \\hat{f}_q\\, c_{ab}\n",
    "= \\hat{f}_0\\,c_a^0 - \\frac{2(\\hat{f}_0 - \\hat{f}_q)c_a^0\\,c_b^0}{K_d+c_a^0+c_b^0 + \\sqrt{\\left(K_d+c_a^0+c_b^0\\right)^2 - 4c_a^0\\,c_b^0}}.\n",
    "\\end{equation}\n",
    "\n",
    ", where $\\hat{f}_0 = f_0 V$ and $\\hat{f}_q = f_q V$ are tranformed parameters. We do not know about $K_d, \\hat{f}_0, \\hat{f}_q$ in Eq.(2), and we have the data for the rest variables, i.e. $c^0_a, c^0_b$. Thus, we set three parameters for this model, namely $K_d, \\hat{f}_0, \\hat{f}_q$. \n",
    "\n",
    "Not considering the outliers, for $K_d$, we only know that it is non-negative, so we use a half normal distribution as the prior for $K_d$, where we want to use a large $\\sigma$ so that the prior is broad enough to cover all the possibilities of $K_d$. For $\\hat{f}_0$ and $\\hat{f}_q$, the product of them and concentration of $a$ should be in the same order of magnitude with the fluorescence readings, since if we do not have any $b$, $F$ would be $\\hat{f}_0 c^a_0$ and if we have a $K_d$ close to zero, $F$ would be $\\hat{f}_q c^a_0$. We also know that $\\hat{f}_0 > \\hat{f}_q$ because firstly it is called 'quenched' and from the data set, generally $F$ decreases as $c^0_b$ increases. Hence, we decide to have broad normal distributions as priors for those two parameters, where the means should be around $10^5 / 50$, and $\\hat{f}_0$ should have a prior that gives values larger than $\\hat{f}_q$ in most cases. Also, we think the measurements should have some noise in them, so we have a normal distriubtion for the final readings with the mean of the calculated $F$ and some $\\sigma$ called noise, which itself is of a half normal distribution. The model is summarized as below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "K_d \\sim \\mbox{HalfNorm}(\\sigma_{K_d}) \\\\\n",
    "\\hat{f}_0 \\sim \\mbox{Norm}(\\mu_{f_0}, \\sigma_{f_0}) \\\\\n",
    "\\hat{f}_q \\sim \\mbox{Norm}(\\mu_{f_q}, \\sigma_{f_q}) \\\\\n",
    "\\sigma \\sim \\mbox{HalfNorm}(\\sigma_{noise}) \\\\\n",
    "F_{temp} = \\hat{f}_0(c_a^0 - c_{ab}) + \\hat{f}_q\\, c_{ab}\n",
    "= \\hat{f}_0\\,c_a^0 - \\frac{2(\\hat{f}_0 - \\hat{f}_q)c_a^0\\,c_b^0}{K_d+c_a^0+c_b^0 + \\sqrt{\\left(K_d+c_a^0+c_b^0\\right)^2 - 4c_a^0\\,c_b^0}} \\\\\n",
    "F \\sim \\mbox{Norm}(F_{temp}, \\sigma).\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then code out the prior predictive check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri_pred_1 = '''data {\n",
    "  // Number of data points\n",
    "  int N;\n",
    "  // conc of a\n",
    "  real ca0;\n",
    "  // conc of b\n",
    "  real cb0[N];\n",
    "  // sigma for Kd\n",
    "  real Kd_sigma;\n",
    "  // mean of f0\n",
    "  real f0_mu;\n",
    "  // sigma of f0\n",
    "  real f0_sigma;\n",
    "  // mean of fq\n",
    "  real fq_mu;\n",
    "  // sigma for fq\n",
    "  real fq_sigma;\n",
    "  // sigma for measurement noise\n",
    "  real noise_sigma;\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "  real Kd;\n",
    "  real f0;\n",
    "  real fq;\n",
    "  // Generated readings\n",
    "  real F[N];\n",
    "  // Calculated F\n",
    "  real temp;\n",
    "  real noise;\n",
    "  \n",
    "  Kd = fabs(normal_rng(0, Kd_sigma));\n",
    "  f0 = normal_rng(f0_mu, f0_sigma);\n",
    "  fq = normal_rng(fq_mu, fq_sigma);\n",
    "  noise = fabs(normal_rng(0, noise_sigma));\n",
    "  \n",
    "  \n",
    "  for (i in 1:N) {\n",
    "  // for every data point, generate the calcualted F first\n",
    "    temp = f0 * ca0 - (2 * (f0 - fq) * ca0 * cb0[i]) / (Kd + ca0 + cb0[i] + sqrt((Kd + ca0 + cb0[i])^2 - 4 * ca0 * cb0[i]));\n",
    "    F[i] = normal_rng(temp, noise);\n",
    "  }\n",
    "}'''\n",
    "\n",
    "sm = bebi103.stan.StanModel(file='hw92_pri_pred.stan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we slice out the concentrations of $b$ and put some numbers for the priors which we think is reasonable in the order of magnitude and broad enough to cover all the possible data. Specially, we try to have a narrower prior for $\\hat{f}_q$ to avoid overlapping of it and $\\hat{f}_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice out the cb0\n",
    "conc_b = df['b conc (nM)'].values\n",
    "\n",
    "# Put reasonable parameters for priors\n",
    "data = dict(N=len(df),\n",
    "            ca0 = 50,\n",
    "            cb0 = conc_b,\n",
    "            Kd_sigma = 150,\n",
    "            f0_mu = 9000,\n",
    "            f0_sigma = 2000,\n",
    "            fq_mu = 4500,\n",
    "            fq_sigma = 1000,\n",
    "            noise_sigma = 10000)\n",
    "\n",
    "# Sample\n",
    "samples_gen = sm.sampling(data=data,\n",
    "                          algorithm='Fixed_param',\n",
    "                          warmup=0,\n",
    "                          chains=1,\n",
    "                          iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take a look at the data in the sampling results to see how we should plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = bebi103.stan.extract_array(samples_gen, name='F')\n",
    "# Take a look\n",
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that for each 'chain_idx', there is a set of data points coresponding to $c^0_b$, so we plot every chain_idx to see what we have from the prior predictive check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize the figure\n",
    "p= bokeh.plotting.figure(width=500,height=400)\n",
    "\n",
    "# Plot vs cb0 for each chain_idx\n",
    "for i in range(100):\n",
    "    p.line(conc_b, df_samples.loc[df_samples['chain_idx'] == i+1, 'F'].values, alpha=0.2)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's kind of hard to tell if there are some unphysical results, but the only thing that we think needs attention here is that $K_d$ should always decrease with incresing $c^0_b$ genearlly, even with the noise, which indicate $\\hat{f}_0 > \\hat{f}_q$, so we try to plot the differences of adjacent elements below to see if they are mostly negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bokeh.plotting.figure(width=500,height=400)\n",
    "\n",
    "for i in range(100):\n",
    "    # Plot vs cb, change the order because of the data format\n",
    "    p.line(conc_b[-2::-1], np.diff(df_samples.loc[df_samples['chain_idx'] == i+1, 'F'].values[::-1]), alpha=0.2, line_width=2)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of them are negative but it is still hard to tell, then we calculate the ECDF for those differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Append differences for each curve\n",
    "    dif = dif + (list(np.diff(df_samples.loc[df_samples['chain_idx'] == i+1, 'F'].values[::-1])))\n",
    "\n",
    "p = bebi103.viz.ecdf(dif)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still about 10% postivie values in the differences, which could be because of the noise, so we sum them up for each curve and plot the ECDF of those sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Append the sum of differences for each curve\n",
    "    dif.append(np.sum(np.diff(df_samples.loc[df_samples['chain_idx'] == i+1, 'F'].values[::-1])))\n",
    "\n",
    "p = bebi103.viz.ecdf(dif)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that there are only four points that are postive, meaning about 4% of the data have  $\\hat{f}_0 < \\hat{f}_q$, which is good to us. Then we proceed to write the code for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, we come up with a way of plotting those data with input variables as below. The way it works is that for every single data point, it compute the medians and some percentiles from the sampling results at that specific point and plot all the medians against the input variable. Initially, we want to use this as a way to plot the prior predictive check but when we do this, we, to some extent, lose information about the shape of the curve. We are not very sure about this, but we assume this plotting method could still inform us of the general trend of the data against the input variables, and could be a good way of illustration. Here, those shades just indicate that some perentage of the curves are lying inside them without giving any information about how the shape of those curves would be, while the medians may be capable of informing us of the shape because all the priors and liklihood are Gaussian distributions or half normal and the medians should give some ideas about what is the most probable values at those points, which should be from the most probable parameters drawn out of the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hw92_predictive(df, x, y=None, namex='index_1', name='F_ppc', perc=[80, 60, 40, 20], \n",
    "                    x_axis_label=None, y_axis_label=None, title=None, plot_width=350, plot_height=225, \n",
    "                    color='blue', data_color=color_palette[1], diff=False):\n",
    "    '''Mimic of predictive ECDF\n",
    "    df - MCMC sampling data frame\n",
    "    x - input variable\n",
    "    y - data\n",
    "    namex - name of the input varible in the data frame\n",
    "    name - name of the predictive results in the data frame\n",
    "    perc - list, default [80, 60, 40, 20]\n",
    "            Percentiles for making colored envelopes for confidence\n",
    "            intervals for the predictive ECDFs. Maximally four can be \n",
    "            specified.'''\n",
    "    \n",
    "    # Initialize the color\n",
    "    if color not in ['green', 'blue', 'red', 'gray',\n",
    "                     'purple', 'orange', 'betancourt']:\n",
    "        raise RuntimeError(\"Only allowed colors are 'green', 'blue', 'red', 'gray', 'purple', 'orange'\")\n",
    "    \n",
    "    colors = {'blue': ['#9ecae1','#6baed6','#4292c6','#2171b5','#084594'],\n",
    "              'green': ['#a1d99b','#74c476','#41ab5d','#238b45','#005a32'],\n",
    "              'red': ['#fc9272','#fb6a4a','#ef3b2c','#cb181d','#99000d'],\n",
    "              'orange': ['#fdae6b','#fd8d3c','#f16913','#d94801','#8c2d04'],\n",
    "              'purple': ['#bcbddc','#9e9ac8','#807dba','#6a51a3','#4a1486'],\n",
    "              'gray': ['#bdbdbd','#969696','#737373','#525252','#252525'],\n",
    "              'betancourt': ['#DCBCBC', '#C79999', '#B97C7C',\n",
    "                             '#A25050', '#8F2727', '#7C0000']}\n",
    "    \n",
    "    # Initialize the figure\n",
    "    p = bokeh.plotting.figure(plot_width=plot_width,\n",
    "                              plot_height=plot_height,\n",
    "                              x_axis_label=x_axis_label,\n",
    "                              y_axis_label=y_axis_label,\n",
    "                              title=title)\n",
    "    \n",
    "    # See if take the diff\n",
    "    if diff:\n",
    "        x = x[1:]\n",
    "        if y is not None:\n",
    "            y = np.diff(y)\n",
    "        Nb = len(x)\n",
    "        y_ppc = np.empty((len(perc) * 2 + 1, Nb))\n",
    "        for i in range(Nb):\n",
    "            temp = df.loc[df[namex]== i+2, name].values - df.loc[df[namex]== i+1, name].values\n",
    "            y_ppc[-1, i] = np.median(temp)\n",
    "            for j in range(len(perc)):\n",
    "                y_ppc[j * 2, i] = np.percentile(temp, 50 - perc[j] / 2)\n",
    "                y_ppc[j * 2 + 1, i] = np.percentile(temp, 50 + perc[j] / 2)\n",
    "    else:                \n",
    "        Nb = len(x)\n",
    "        y_ppc = np.empty((len(perc) * 2 + 1, Nb))\n",
    "        # For each data point, take all the sampling results at this point\n",
    "        for i in range(Nb):\n",
    "            temp = df.loc[df[namex]== i+1, name].values\n",
    "            # Find the median and corresponding percentiles\n",
    "            y_ppc[-1, i] = np.median(temp)\n",
    "            for j in range(len(perc)):\n",
    "                y_ppc[j * 2, i] = np.percentile(temp, 50 - perc[j] / 2)\n",
    "                y_ppc[j * 2 + 1, i] = np.percentile(temp, 50 + perc[j] / 2)\n",
    "    \n",
    "    # Plotting like predictive_ecdf\n",
    "    for j in range(len(perc)):\n",
    "        bebi103.viz.fill_between(x, y_ppc[j * 2, :],\n",
    "                     x, y_ppc[j * 2 + 1,:],\n",
    "                     p=p,\n",
    "                     show_line=False,\n",
    "                     fill_color=colors[color][j])\n",
    "        \n",
    "    p.line(x, y_ppc[-1, :],\n",
    "           line_width=2,\n",
    "           color=colors[color][-1])\n",
    "    \n",
    "    if y is not None:\n",
    "        p.line(x, y, line_width=2, color='orange')\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using the function above\n",
    "p1 = hw92_predictive(df_samples, conc_b, name='F', perc=[99, 75, 50, 25], diff=False, plot_width=500, plot_height=400)\n",
    "\n",
    "bokeh.io.show(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty clear that this way of plotting does not show much information about the possible shapes of the curve though some. With results from the same prior predictive check, it loses the track of those curves with wrong trends. However, we again think it is still a good way to show where most of the data are, because we suppose if one has a wrong prior giving lots of unphysical values, they will still show up. To test that, we can change some parameters in the prior predictive check, for instance, we change the mean of $\\hat{f}_0$ to 7000 which should give some overlapping with $\\hat{f}_q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice out the cb0\n",
    "conc_b = df['b conc (nM)'].values\n",
    "\n",
    "# Put reasonable parameters for priors\n",
    "data = dict(N=len(df),\n",
    "            ca0 = 50,\n",
    "            cb0 = conc_b,\n",
    "            Kd_sigma = 150,\n",
    "            f0_mu = 7000,\n",
    "            f0_sigma = 2000,\n",
    "            fq_mu = 4500,\n",
    "            fq_sigma = 1000,\n",
    "            noise_sigma = 10000)\n",
    "\n",
    "# Sample\n",
    "samples_gen = sm.sampling(data=data,\n",
    "                          algorithm='Fixed_param',\n",
    "                          warmup=0,\n",
    "                          chains=1,\n",
    "                          iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take a look at the data in the sampling results to see how we should plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = bebi103.stan.extract_array(samples_gen, name='F')\n",
    "# Take a look\n",
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that for each 'chain_idx', there is a set of data points coresponding to $c^0_b$, so we plot every chain_idx to see what we have from the prior predictive check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize the figure\n",
    "p= bokeh.plotting.figure(width=500,height=400)\n",
    "\n",
    "# Plot vs cb0 for each chain_idx\n",
    "for i in range(100):\n",
    "    p.line(conc_b, df_samples.loc[df_samples['chain_idx'] == i+1, 'F'].values, alpha=0.2)\n",
    "    \n",
    "# Plot using the function above\n",
    "p1 = hw92_predictive(df_samples, conc_b, name='F', perc=[99, 75, 50, 25], diff=False, plot_width=500, plot_height=400)\n",
    "\n",
    "bokeh.io.show(bokeh.layouts.gridplot([[p, p1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the right figure, the trend does show up in the 99% shades, while other shades indicate most of the curves are of the right shapes. Looking at those two plots, we believe this way to plot predictive check is somewhat informative and clear, especially when one has a lot of iterations when sampling. We will keep using this function through HW9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code_normal = \"\"\"\n",
    "data {\n",
    "  // Number of datapoints\n",
    "  int N;\n",
    "  // Conc of a\n",
    "  int ca0;\n",
    "  // Conc of b\n",
    "  real cb0[N];\n",
    "  // Measured fluorescence\n",
    "  real F[N];\n",
    "}\n",
    "\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> Kd;\n",
    "  real<lower=0> f0;\n",
    "  real<lower=0> fq;\n",
    "  real<lower=0> noise;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  real F_temp[N];\n",
    "  for (i in 1:N) {\n",
    "  // Generate calculated F for each point\n",
    "    F_temp[i] = f0 * ca0 - (2 * (f0 - fq) * ca0 * cb0[i]) / (Kd + ca0 + cb0[i] + sqrt((Kd + ca0 + cb0[i])^2 - 4 * ca0 * cb0[i]));\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "model {\n",
    "  Kd ~ normal(0, 100);\n",
    "  f0 ~ normal(9000, 2000);\n",
    "  fq ~ normal(4500, 1000);\n",
    "  noise ~ normal(0, 10000);\n",
    "  \n",
    "  F ~ normal(F_temp, noise);\n",
    "}\n",
    "\n",
    "\n",
    "generated quantities {\n",
    "  // Posterior predictive check\n",
    "  real F_ppc[N];\n",
    "  \n",
    "  for (i in 1:N) {\n",
    "    F_ppc[i] = normal_rng(F_temp[i], noise);\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stan code is attached above for reference, but the model is compiled from the standalone file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie from the standalone file\n",
    "sm_normal = bebi103.stan.StanModel(file='hw92_normal.Stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data\n",
    "data = dict(N=len(df),\n",
    "            ca0 = 50,\n",
    "            cb0 = conc_b,\n",
    "            F = df['fluorescence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample out of the model\n",
    "samples_normal = sm_normal.sampling(data=data)\n",
    "# Run diagnostics\n",
    "bebi103.stan.check_all_diagnostics(samples_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagnostics look good, so we plot the corner plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(bebi103.viz.corner(samples_normal, \n",
    "                                 pars=['Kd', 'f0', 'fq','noise'],\n",
    "                                 plot_width=200,\n",
    "                                 cmap='gray',\n",
    "                                 alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also marginalize to show the ECDF of those three parameters respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = bebi103.stan.to_dataframe(samples_normal)\n",
    "\n",
    "plots = [bebi103.viz.ecdf(df_normal[param], x_axis_label=param, plot_height=200, plot_width=250) \n",
    "                 for param in ['Kd', 'f0', 'fq']]\n",
    "                                      \n",
    "bokeh.io.show(bokeh.layouts.gridplot(plots, ncols=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well the model does, we want to plot the results from posterior predictive check, where we want to use the function mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the posterior predictive results\n",
    "df_samples_ppc = bebi103.stan.extract_array(samples_normal, name='F_ppc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = hw92_predictive(df_samples_ppc, conc_b, df['fluorescence'].values, perc=[99, 70, 50, 25], name='F_ppc', plot_width=500, plot_height=400, title='Normal')\n",
    "\n",
    "bokeh.io.show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not bad. Although there are about two outliers, they are both in the 99% range and one of those is within 70%. For the rest data point, we can see that the curve of medians fits well with the measured data. To consider those outliers, we change the likelihood to student-t distribution, so the model becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "K_d \\sim \\mbox{HalfNorm}(\\sigma_{K_d}) \\\\\n",
    "\\hat{f}_0 \\sim \\mbox{Norm}(\\mu_{f_0}, \\sigma_{f_0}) \\\\\n",
    "\\hat{f}_q \\sim \\mbox{Norm}(\\mu_{f_q}, \\sigma_{f_q}) \\\\\n",
    "\\sigma \\sim \\mbox{HalfNorm}(\\sigma_{noise}) \\\\\n",
    "\\nu \\sim \\mbox{HalfNorm}(1, 100) \\\\\n",
    "F_{temp} = \\hat{f}_0(c_a^0 - c_{ab}) + \\hat{f}_q\\, c_{ab}\n",
    "= \\hat{f}_0\\,c_a^0 - \\frac{2(\\hat{f}_0 - \\hat{f}_q)c_a^0\\,c_b^0}{K_d+c_a^0+c_b^0 + \\sqrt{\\left(K_d+c_a^0+c_b^0\\right)^2 - 4c_a^0\\,c_b^0}} \\\\\n",
    "F \\sim \\mbox{Student-t}(F_{temp}, \\sigma, \\nu).\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code_t = \"\"\"\n",
    "data {\n",
    "  // Number of datapoints\n",
    "  int N;\n",
    "  // Conc of a\n",
    "  int ca0;\n",
    "  // Conc of b\n",
    "  real cb0[N];\n",
    "  // Measured fluorescence\n",
    "  real F[N];\n",
    "}\n",
    "\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> Kd;\n",
    "  real<lower=0> f0;\n",
    "  real<lower=0> fq;\n",
    "  real<lower=1> nu;\n",
    "  real<lower=0> noise;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  real F_temp[N];\n",
    "  for (i in 1:N) {\n",
    "  // Generate calculated F for each point\n",
    "    F_temp[i] = f0 * ca0 - (2 * (f0 - fq) * ca0 * cb0[i]) / (Kd + ca0 + cb0[i] + sqrt((Kd + ca0 + cb0[i])^2 - 4 * ca0 * cb0[i]));\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "model {\n",
    "  Kd ~ normal(0, 100);\n",
    "  f0 ~ normal(9000, 2000);\n",
    "  fq ~ normal(4500, 1000);\n",
    "  noise ~ normal(0, 10000);\n",
    "  nu ~ normal(1,100);\n",
    "  \n",
    "  F ~ student_t(nu, F_temp, noise);\n",
    "}\n",
    "\n",
    "\n",
    "generated quantities {\n",
    "  real F_ppc[N];\n",
    "  \n",
    "  // Posterior predictive check\n",
    "  for (i in 1:N) {\n",
    "    F_ppc[i] = student_t_rng(nu, F_temp[i], noise);\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stan code is attached above for reference, but the model is compiled from the standalone file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie from the standalone file\n",
    "sm_t = bebi103.stan.StanModel(file='hw92_student_t.Stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the same data\n",
    "samples_t = sm_t.sampling(data=data)\n",
    "# Run diagnostics\n",
    "bebi103.stan.check_all_diagnostics(samples_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good, we take a look at the corner plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(bebi103.viz.corner(samples_t, \n",
    "                                 pars=['Kd', 'f0', 'fq','noise','nu'],\n",
    "                                 plot_width=200,\n",
    "                                 cmap='gray',\n",
    "                                 alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We marginalize to show the ECDF of those three parameters and compare them with those from the normal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = bebi103.stan.to_dataframe(samples_t)\n",
    "\n",
    "plots_t = [bebi103.viz.ecdf(df_t[param], x_axis_label=param, plot_height=200, plot_width=250) \n",
    "                 for param in ['Kd', 'f0', 'fq']]\n",
    "                                      \n",
    "bokeh.io.show(bokeh.layouts.gridplot(plots_t + plots, ncols=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look pretty similar, and we check the posterior predictive results and compare it with that from normal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_ppc_t = bebi103.stan.extract_array(samples_t, name='F_ppc')\n",
    "\n",
    "p3 = hw92_predictive(df_samples_ppc_t, conc_b, df['fluorescence'].values, perc=[99, 70, 50, 25], name='F_ppc', plot_width=500, plot_height=400, title='Student-t')\n",
    "\n",
    "bokeh.io.show(bokeh.layouts.gridplot([[p2, p3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't think there is a significant different in term of the posterior predicitve check. The 99% range in the student-t likelihood is slightly larger without compromising the medians and smaller percentile ranges, which might be the advantage of considering the outliers. We move on and try the good-bad data model, which is shown as below. The prior for the weight is chosen to be a beta distribution that gives a slight preference for good data since we think most of the data points should be good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather}\n",
    "K_d \\sim \\mbox{HalfNorm}(\\sigma_{K_d}) \\\\\n",
    "\\hat{f}_0 \\sim \\mbox{Norm}(\\mu_{f_0}, \\sigma_{f_0}) \\\\\n",
    "\\hat{f}_q \\sim \\mbox{Norm}(\\mu_{f_q}, \\sigma_{f_q}) \\\\\n",
    "\\sigma \\sim \\mbox{HalfNorm}(\\sigma_{noise}) \\\\\n",
    "\\sigma_{bad} \\sim \\mbox{HalfNorm}(\\sigma_{noise}) \\mbox{ with } \\sigma_{bad} > \\sigma\\\\\n",
    "w_i \\sim \\mbox{Beta}(3,2) \\\\\n",
    "F_{i, temp} = \\hat{f}_0(c_a^0 - c_{ab}) + \\hat{f}_q\\, c_{ab}\n",
    "= \\hat{f}_0\\,c_a^0 - \\frac{2(\\hat{f}_0 - \\hat{f}_q)c_a^0\\,c_b^0}{K_d+c_a^0+c_b^0 + \\sqrt{\\left(K_d+c_a^0+c_b^0\\right)^2 - 4c_a^0\\,c_b^0}} \\\\\n",
    "F_i \\sim w_i \\mbox{Norm}(F_{i, temp}, \\sigma) + (1 - w_i) \\mbox{Norm}(F_{i, temp}, \\sigma_{bad}).\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code_mix = \"\"\"\n",
    "data {\n",
    "  // Number of datapoints\n",
    "  int N;\n",
    "  // Conc of a\n",
    "  int ca0;\n",
    "  // Conc of b\n",
    "  real cb0[N];\n",
    "  // Measured fluorescence\n",
    "  real F[N];\n",
    "}\n",
    "\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> Kd;\n",
    "  real<lower=0> f0;\n",
    "  real<lower=0> fq;\n",
    "  positive_ordered[2] noise;\n",
    "  real<lower=0, upper=1> w[N];\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  real F_temp[N];\n",
    "  for (i in 1:N) {\n",
    "  // Generate calculated F for each point\n",
    "    F_temp[i] = f0 * ca0 - (2 * (f0 - fq) * ca0 * cb0[i]) / (Kd + ca0 + cb0[i] + sqrt((Kd + ca0 + cb0[i])^2 - 4 * ca0 * cb0[i]));\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "model {\n",
    "  Kd ~ normal(0, 100);\n",
    "  f0 ~ normal(9000, 2000);\n",
    "  fq ~ normal(4500, 1000);\n",
    "  noise ~ normal(0, 10000);\n",
    "  w ~ beta(3,2);\n",
    "  \n",
    "  for (i in 1:N) {\n",
    "    target += log_mix(w[i],\n",
    "                      normal_lpdf(F[i] | F_temp[i], noise[1]),\n",
    "                      normal_lpdf(F[i] | F_temp[i], noise[2]));\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  real F_ppc[N];\n",
    "  \n",
    "  // Posterior predictive check\n",
    "  for (i in 1:N) {\n",
    "    if (uniform_rng(0.0, 1.0) < w[i]) {\n",
    "      F_ppc[i] = normal_rng(F_temp[i], noise[1]);\n",
    "    }\n",
    "    else {\n",
    "      F_ppc[i] = normal_rng(F_temp[i], noise[2]);\n",
    "    }    \n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stan code is attached above for reference, but the model is compiled from the standalone file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie from the standalone file\n",
    "sm_mix = bebi103.stan.StanModel(file='hw92_mix.Stan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very few iterations with divergences in this model, so we use a larger adapt_delta instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample with the same data\n",
    "samples_mix = sm_mix.sampling(data=data, control=dict(adapt_delta=0.96))\n",
    "# Run diagnostics\n",
    "bebi103.stan.check_all_diagnostics(samples_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is good, and we plot the corner plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(bebi103.viz.corner(samples_mix, \n",
    "                                 pars=['Kd', 'f0', 'fq','noise[1]','noise[2]'],\n",
    "                                 plot_width=200,\n",
    "                                 cmap='gray',\n",
    "                                 alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be some differences but no significant ones in the values of $K_d$. To make sure, we plot the ECDF for $K_d$ sampled out of three models together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mix = bebi103.stan.to_dataframe(samples_mix)\n",
    "\n",
    "pc = bebi103.viz.ecdf(df_normal['Kd'], x_axis_label='K_d', plot_height=300, plot_width=400, color=color_palette[0])\n",
    "pc = bebi103.viz.ecdf(df_t['Kd'], color=color_palette[1], p=pc)\n",
    "pc = bebi103.viz.ecdf(df_mix['Kd'], color=color_palette[2], p=pc)\n",
    "\n",
    "bokeh.io.show(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are almost the same. We then plot the posterior predictive results from three of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_ppc_mix = bebi103.stan.extract_array(samples_mix, name='F_ppc')\n",
    "\n",
    "p4 = hw92_predictive(df_samples_ppc_mix, conc_b, df['fluorescence'].values, perc=[99, 70, 50, 25], name='F_ppc', plot_width=500, plot_height=400, title='Good-bad')\n",
    "\n",
    "bokeh.io.show(bokeh.layouts.gridplot([[p2, p3, p4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, they look pretty much the same, while the Good-bad data model looks nicer to some extent where the smaller percentiles are tighter without compromising most of the fitting, while the 99% range covers the outliers, but as in the parameter estimates, they are almost the same. Finally, we try to put the estimates of $K_d$ together and compare them. We firstly make them into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the parameters\n",
    "pars = ['Kd', 'f0', 'fq']\n",
    "# Initialize the data frame\n",
    "df_pars = pd.DataFrame()\n",
    "# List the range of those parameters\n",
    "for parm in pars:\n",
    "    sample_temp = samples_normal.extract(parm)[parm]\n",
    "    df_pars = df_pars.append(pd.DataFrame({'parameter':[parm],\n",
    "                             'method':['normal'],\n",
    "                             'low':[np.percentile(sample_temp, 2.5)],\n",
    "                             'middle':[np.median(sample_temp)],\n",
    "                             'high':[np.percentile(sample_temp, 97.5)]}),\n",
    "                            ignore_index=True)\n",
    "\n",
    "for parm in pars:\n",
    "    sample_temp = samples_t.extract(parm)[parm]\n",
    "    df_pars = df_pars.append(pd.DataFrame({'parameter':[parm],\n",
    "                             'method':['student'],\n",
    "                             'low':[np.percentile(sample_temp, 2.5)],\n",
    "                             'middle':[np.median(sample_temp)],\n",
    "                             'high':[np.percentile(sample_temp, 97.5)]}),\n",
    "                            ignore_index=True)\n",
    "    \n",
    "for parm in pars:\n",
    "    sample_temp = samples_mix.extract(parm)[parm]\n",
    "    df_pars = df_pars.append(pd.DataFrame({'parameter':[parm],\n",
    "                             'method':['good_bad'],\n",
    "                             'low':[np.percentile(sample_temp, 2.5)],\n",
    "                             'middle':[np.median(sample_temp)],\n",
    "                             'high':[np.percentile(sample_temp, 97.5)]}),\n",
    "                            ignore_index=True)\n",
    "\n",
    "df_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we borrow the code from the tutorial, plot out the estimates of $K_d$ out of three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering of y-axis\n",
    "order = [(g, m) for g in ['Kd'] for m in ['normal', 'student', 'good_bad']]\n",
    "\n",
    "# Build data source and color factors for plots\n",
    "grouped = df_pars.groupby(['parameter', 'method'])\n",
    "cat_range, factors, color_factors = bebi103.viz._get_cat_range(\n",
    "    df_pars, grouped, order, 'parameter', True)\n",
    "source = bebi103.viz._cat_source(df_pars, ['parameter', 'method'], list(df_pars.columns), 'parameter')\n",
    "color = bokeh.transform.factor_cmap('parameter',\n",
    "                                     palette=['#f28e2b', '#e15759', '#4e79a7'],\n",
    "                                     factors=color_factors)\n",
    "\n",
    "# Make plots\n",
    "p = bokeh.plotting.figure(y_range=cat_range, plot_height=300)\n",
    "p.circle(source=source, x='middle', y='cat', color=color)\n",
    "p.segment(source=source, y0='cat', y1='cat', x0='low', x1='high', color=color)\n",
    "\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we think the results are almost the same no matter whether we try to detect outliers or not. There might be some better results in the good-bad data model, but generally we think for data set like this which has only a few outliers, the detection of outliers would not improve the model much and if much more efforts are needed for those detections, we think it is not worth doing, while if the data set is large and one is not sure how many outliers there are, embeding some outlier detections may be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -v -p numpy,scipy,bokeh,jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
