{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6.2: Least squares (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: Zhiyang did most of this problem and the whole group discuss together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**\n",
    "\n",
    "As described in the problem, we assume the measurement of $y$ are i.i.d. and Gaussian distributed, which can be written in likelihood form as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(y_i\\ |\\ f_y(x_i;\\theta), \\sigma_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\exp \\left[ -\\frac{(y_i - f_y(x_i;\\theta))^2}{2 \\sigma_i^2} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the joint distribution of all the measurments of y gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(y\\ |\\ f_y(x_i;\\theta), \\sigma) = \\prod^N_{i=1} \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\exp \\left[ -\\frac{(y_i - f_y(x_i;\\theta))^2}{2 \\sigma_i^2} \\right] =  \\prod^N_{i=1} \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\cdot \\exp \\left[ -\\sum^N_{i=1} \\frac{(y_i - f_y(x_i;\\theta))^2}{2 \\sigma_i^2}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since there are uniform priors on all parameters and vlaues of $\\sigma_i$ are usually known, so they are not considered as parameters to be estimated. To conduct MAP, we wil need have the posterior, and the priors here are uniform distributed, we may assume it as a constant through the parameter range which is unknown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g(\\theta) = \\frac{1}{b - a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", where $a$ and $b$ is the lower and upper bound, if any, of the parameter $\\theta$. Then, we can have the posterior as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(\\theta\\ |\\ y) = \\frac{ \\prod^N_{i=1} \\frac{1}{\\sqrt{2 \\pi \\sigma_i^2}} \\cdot \\exp \\left[ -\\sum^N_{i=1} \\frac{(y_i - f_y(x_i;\\theta))^2}{2 \\sigma_i^2}\\right]}{(b-a) f(y)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP is basically look for $\\theta$ that gives the largest $g(\\theta\\ |\\ y)$, and since the denominator is irrelevant to $\\theta$ so we should actually looking for maximum of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\exp \\left[ -\\sum^N_{i=1} \\frac{(y_i - f_y(x_i;\\theta))^2}{2 \\sigma_i^2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", which is equivalent to finding minimum of the term below, since the exponential is monotonic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation} \\sum^N_{i=1} \\frac{(y_i - f_y(x_i;\\theta))^2}{\\sigma_i^2} \\tag{1} \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", which is equivalent to $\\theta$ that minimize the sum of the squares of the residuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "When we assume homoscedastic errors, i.e. $\\sigma_i = \\sigma$, equation (1) would become:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}  \\frac{\\sum^N_{i=1}(y_i - f_y(x_i;\\theta))^2}{\\sigma^2}   \\tag{2}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", where the value of $\\sigma$ no more affects finding maximum of this term, so we only need to find the maximum of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum^N_{i=1}(y_i - f_y(x_i;\\theta))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", which means we do not need to consider $\\sigma$ when we do the MAP or, equivalently, the least squares regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "First of all, this approach is basically MAP, so it has all the disadvantages that MAP has. MAP is a single-point summary of the posterior near certain initial point, where we lose all the information about the shape of the posterior and we are not sure whether the inital point is a good one or not. Also, if we try to compute the credible region for MAP, we may fail to properly describe the posterior by Gaussian approximation if the posterior distribution around the MAP point is not that smooth. \n",
    "\n",
    "Secondly, the uniform prior assumption can be problematic. It fails to pose some simple constraints on the parameters to avoid unphysical values, and it does not give any information about the parameters. When assuming uniform prior through the real numbers, it would also lead to problems like that it is difficult, if not impossible, to implement that. We should definitely have some informative priors if possible.\n",
    "\n",
    "Also, when $\\sigma_i$ is not known, it might be a better idea to have another prior for $\\sigma_i$ instead of assuming homoscedastic errors, though in some cases we think homoscedastic error assumption is good enough and can save some computional efforts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
